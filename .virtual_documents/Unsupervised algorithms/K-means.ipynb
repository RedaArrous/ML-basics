%matplotlib widget
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


plt.style.use("fivethirtyeight")


from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Parameters
n_samples = 1400    # number of points
n_features = 2     # 2D points
n_clusters = 3     # number of clusters
random_state = 42  # for reproducibility

# Generate the dataset
original_X, y = make_blobs(
    n_samples=n_samples,
    n_features=n_features,
    centers=n_clusters,
    cluster_std=1.0,  # spread of clusters
    random_state=random_state
)


X = original_X.copy()


print('X shape:', original_X.shape)
print('y shape:', y.shape)





plt.figure(figsize=(8, 6))
# Use viridis colormap to match the second plot
import matplotlib.pyplot as plt
viridis = plt.get_cmap('viridis')
colors = [viridis(i/(n_clusters-1)) for i in range(n_clusters)]

for cluster in range(n_clusters):
    plt.scatter(
        original_X[y == cluster, 0],
        original_X[y == cluster, 1],
        color=colors[cluster],
        label=f'Cluster {cluster}',
        alpha=0.7,
    )
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('2D K-means Dataset')
plt.legend()
plt.tight_layout()
plt.show()














centroids = np.array([ [2.5, 10], [5., -4.],[-7.5, 0]])
print(f"centroids:\n {centroids}")
print(f"centroids shape: {centroids.shape}")


plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    original_X[:, 0], original_X[:, 1],
    c=y, cmap='viridis', alpha=0.6, marker='o'
)

# Plot centroids with matching cluster colors
plt.scatter(
    centroids[:, 0], centroids[:, 1],
    c='r',
    marker='X', s=250, edgecolor='black', linewidth=1.5,
    label='Centroids'
)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset with Initial Centroids')
plt.legend()
plt.tight_layout()
plt.show()





def assign_to_centroids(X, cntr):
    """
    Assign each point in X to the closest centroid.

    Parameters
    ----------
    X : ndarray of shape (m, n)
        The set of points to work with, where m is the number of points 
        and n is the number of features.
    cntr : ndarray of shape (k, n)
        The set of k centroids.

    Returns
    -------
    c : ndarray of shape (m,)
        Array where c[i] holds the index of the centroid assigned to X[i].
    """

    # Compute squared distances between each point and each centroid,
    # then assign each point to the nearest centroid
    c = np.argmin(np.sum((cntr[np.newaxis, :, :] - X[:, np.newaxis, :])**2, axis=2), axis=1)

    return c












c = assign_to_centroids(X, centroids)
values, counts = np.unique(c, return_counts=True)

print(values)
print(counts)





def update_centroids(X, cntr, c):
    """
    Recompute the centroids based on the current point assignments.

    Parameters
    ----------
    X : ndarray of shape (m, n)
        The set of points, where m is the number of points 
        and n is the number of features.
    cntr : ndarray of shape (k, n)
        The current set of k centroids.
    c : ndarray of shape (m,)
        Array of centroid indices, where c[i] is the index of 
        the centroid assigned to point X[i].

    Returns
    -------
    new_cntr : ndarray of shape (k, n)
        The updated centroids, where each centroid is the mean of 
        the points assigned to it.
    """

    new_cntr = np.zeros(cntr.shape)
    for i in range(cntr.shape[0]):
        if np.sum(c == i) == 0:
            
        new_cntr[i] = np.mean(X[c == i] , axis=0)
        
    return new_cntr











def cost_func(X, cntr, c):
    """
    Compute the K-means cost (mean squared distance to assigned centroids).

    Parameters
    ----------
    X : ndarray of shape (m, n)
        Array of data points, where m is the number of points and n is the number of features.
    cntr : ndarray of shape (k, n)
        Array of centroids, where k is the number of clusters.
    c : ndarray of shape (m,)
        Array of centroid indices assigned to each point in X.

    Returns
    -------
    j : float
        The mean squared distance between each point and its assigned centroid (K-means cost).
    """
    j = np.mean(np.sum((X - cntr[c])**2, axis=1))
    return j


cost_func(X, centroids, c)








def K_means(X, cntr, iters=10, epsilon=0.005, verbose=True):
    """
    Run the K-means clustering algorithm on a dataset.

    Parameters
    ----------
    X : ndarray of shape (m, n)
        Dataset of m points with n features each.
    cntr : ndarray of shape (k, n)
        Initial centroids for the k clusters.
    iters : int, default=10
        Maximum number of iterations to run the algorithm.
    epsilon : float, default=0.005
        Convergence tolerance. If the decrease in cost between 
        consecutive iterations is less than or equal to epsilon, 
        the algorithm stops early.
    verbose : bool, default=True
        If True, prints progress information such as cost values and 
        centroid updates during training.

    Returns
    -------
    cntr : ndarray of shape (k, n)
        Final centroids after clustering.
    c : ndarray of shape (m,)
        Cluster assignment for each point in X.
    cost : list of float
        History of cost values (mean squared distances) at each iteration.
    
    Notes
    -----
    - Uses the helper functions `assign_to_centroids`, `update_centroids`, 
      and `cost_func`.
    - Stops early if the cost converges within the given epsilon tolerance.
    """
    # For visualization and stopping when converging
    print_count = max(1, iters // 10)

    c = assign_to_centroids(X, cntr)
    cost = [cost_func(X, cntr, c)]

    if verbose:
        print("Initial cost =", cost[0])
        print("Initial centroids :\n", cntr)
        print("#" * 30)

    for i in range(iters):
        c = assign_to_centroids(X, cntr)
        cntr = update_centroids(X, cntr, c)
        cost.append(cost_func(X, cntr, c))
        
        if verbose and i % print_count == 0: 
            print(f"Iteration {i+1}:\ncost = {cost[-1]:.6f}")
            print(f"Centroids:\n{cntr}\n")
            
        if cost[-2] - cost[-1] <= epsilon:
            if verbose:
                print(f"Converged at iteration {i+1}")
            break
            
    return cntr, c, cost


centroids, c, cost = K_means(X, centroids)


plt.figure(figsize=(8, 6))
plt.plot(range(len(cost)), cost, marker='o', linewidth=2, markersize=4)
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('K-means Convergence: Cost vs Iteration')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    original_X[:, 0], original_X[:, 1],
    c=y, cmap='viridis', alpha=0.6, marker='o'
)

# Plot centroids with matching cluster colors
plt.scatter(
    centroids[:, 0], centroids[:, 1],
    c='r',
    marker='X', s=250, edgecolor='black', linewidth=1.5,
    label='Centroids'
)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset with Initial Centroids')
plt.legend()
plt.tight_layout()
plt.show()











bad_centroids = np.array([[5, 5], [5.2, 5.1], [10, 0]])


plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    original_X[:, 0], original_X[:, 1],
    c=y, cmap='viridis', alpha=0.6, marker='o'
)

# Plot bad centroids
plt.scatter(
    bad_centroids[:, 0], bad_centroids[:, 1],
    c='r',
    marker='X', s=250, edgecolor='black', linewidth=1.5,
    label='Bad Centroids'
)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset with Badly Initialized Centroids')
plt.legend()
plt.tight_layout()
plt.show()


bad_centroids, bad_c, bad_cost = K_means(X, bad_centroids)


plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    original_X[:, 0], original_X[:, 1],
    c=y, cmap='viridis', alpha=0.6, marker='o'
)

# Plot bad centroids
plt.scatter(
    bad_centroids[:, 0], bad_centroids[:, 1],
    c='r',
    marker='X', s=250, edgecolor='black', linewidth=1.5,
    label='Bad Centroids'
)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset with Badly Initialized Centroids')
plt.legend()
plt.tight_layout()
plt.show()





plt.figure(figsize=(8, 6))

# Plot bad initialization
plt.plot(
    range(len(bad_cost)), bad_cost,
    marker='o', linewidth=2, markersize=4,
    color='red', label='Bad Initialization'
)

# Plot good initialization
plt.plot(
    range(len(cost)), cost,
    marker='o', linewidth=2, markersize=4,
    color='#21a191', label='Good Initialization'
)

plt.xlabel('Iteration')
plt.ylabel('Cost (Inertia)')
plt.title('K-means Convergence: Good vs. Bad Initialization')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


















def K_pp(X, k, random_state=None):
    """
    Initialize centroids using the K-means++ algorithm.

    Parameters
    ----------
    X : ndarray of shape (m, n)
        Dataset of m points with n features each.
    k : int
        Number of centroids to initialize.

    Returns
    -------
    cntr : ndarray of shape (k, n)
        Initialized centroids chosen by the K-means++ procedure.
    """
    m, n = X.shape
    if random_state is not None:
        np.random.seed(random_state)

    # 1. Pick the first centroid randomly
    random_index = np.random.choice(m)
    cntr = X[random_index][np.newaxis, :]  # shape (1, n)

    # 2. Pick the remaining centroids
    for _ in range(1, k):
        # distance from each point to its closest centroid
        distances = np.min(
            np.sum((X[:, np.newaxis, :] - cntr[np.newaxis, :, :]) ** 2, axis=2),
            axis=1
        )

        # probability distribution (proportional to squared distance)
        probability = distances / np.sum(distances)

        # choose new centroid index
        new_cntr_index = np.random.choice(m, p=probability)
        new_cntr = X[new_cntr_index][np.newaxis, :]  # keeps it a 2-D array

        # stack it with existing centroids
        cntr = np.vstack([cntr, new_cntr])

    return cntr





centroids_kpp = K_pp(X, k=3)
centroids_kpp


plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    original_X[:, 0], original_X[:, 1],
    c=y, cmap='viridis', alpha=0.6, marker='o'
)

# Plot centroids with matching cluster colors
plt.scatter(
    centroids_kpp[:, 0], centroids_kpp[:, 1],
    c='r',  # cluster index 0,1,2
    marker='X', s=250, edgecolor='black', linewidth=1.5,
    label='Centroids'
)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Dataset with Initial Centroids')
plt.legend()
plt.tight_layout()
plt.show()





def K_means(X, cntr=None, k=1, iters=10, random_state=None, epsilon=0.005, verbose=True):
    """
    Run the K-means clustering algorithm on a dataset.

    Parameters
    ----------
    X : ndarray of shape (m, n)
        Dataset of m points with n features each.
    cntr : ndarray of shape (k, n), default=None
        Initial centroids for the k clusters.
        If None, the algorithm initializes centroids using the K-means++ algorithm.
    k : int, default=1
        Number of clusters.
    iters : int, default=10
        Maximum number of iterations to run the algorithm.
    epsilon : float, default=0.005
        Convergence tolerance. If the decrease in cost between 
        consecutive iterations is less than or equal to epsilon, 
        the algorithm stops early.
    verbose : bool, default=True
        If True, prints progress information such as cost values and 
        centroid updates during training.

    Returns
    -------
    cntr : ndarray of shape (k, n)
        Final centroids after clustering.
    c : ndarray of shape (m,)
        Cluster assignment for each point in X.
    cost : list of float
        History of cost values (mean squared distances) at each iteration.
    
    Notes
    -----
    - Uses the helper functions `assign_to_centroids`, `update_centroids`, 
      and `cost_func`.
    - Stops early if the cost converges within the given epsilon tolerance.
    """
    # For visualization and convergence checks
    print_count = max(1, iters // 10)

    # Initialize centroids
    if cntr is None:
        cntr = K_pp(X=X, k=k, random_state=random_state)

    # First assignment and initial cost
    c = assign_to_centroids(X, cntr)
    cost = [cost_func(X, cntr, c)]

    if verbose:
        print("Initial cost =", cost[0])
        print("Initial centroids:\n", cntr)
        print("#" * 30)

    for i in range(iters):
        c = assign_to_centroids(X, cntr)
        cntr = update_centroids(X, cntr, c)
        cost.append(cost_func(X, cntr, c))
        
        if verbose and i % print_count == 0: 
            print(f"Iteration {i+1}:\ncost = {cost[-1]:.6f}")
            print(f"Centroids:\n{cntr}\n")
            
        if abs(cost[-2] - cost[-1]) <= epsilon:
            if verbose:
                print(f"Converged at iteration {i+1}")
            break
            
    return cntr, c, cost






centroids_kpp, c, K_pp_cost = K_means(X, k=3, random_state=42)





plt.figure(figsize=(8, 6))

# Plot bad initializations
plt.plot(
    range(len(bad_cost)), bad_cost,
    marker='o', linewidth=2, markersize=4,
    color='#d9534f', label='Very bad Initialization'
)

plt.plot(
    range(len(cost)), cost,
    marker='o', linewidth=2, markersize=4,
    color='#21a191', label='Manually chosen initialization'
)


# Plot good initialization

plt.plot(
    range(len(K_pp_cost)), K_pp_cost,
    marker='o', linewidth=2, markersize=4,
    color='#f0ad4e', label='K-means++ Initialization'
)


plt.xlabel('Iteration')
plt.ylabel('Cost (Inertia)')
plt.title('K-means Convergence: Good vs. Bad Initialization')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()





















K_cost = list()
number_of_k = 20
for k in range(1, number_of_k + 1):
    centr, c_, cos = K_means(X, k=k, random_state=42, verbose=False)

    K_cost.append(cos[-1])


plt.figure(figsize=(10, 6))
plt.plot(range(1, number_of_k + 1), K_cost, marker='o', linestyle='-')
plt.xlabel("K (number of clusters)")
plt.ylabel("Cost (SSE)")
plt.title("Elbow Method: Cost vs. K")
plt.grid(True)

# force x-axis to show only integer ticks
plt.xticks(range(1, number_of_k + 1))
plt.tight_layout()
plt.show()











def silhouette(X, cntr, c=None):
    """
    Compute the mean silhouette score for a clustering.

    The silhouette score measures how similar each point is to its own cluster
    (cohesion) compared to other clusters (separation).

    Parameters
    ----------
    X : ndarray of shape (m, n)
        The dataset with m samples and n features.
    cntr : ndarray of shape (k, n)
        The centroids of the k clusters.
    c : ndarray of shape (m,), optional
        Precomputed cluster assignments for each sample. If None, assignments
        will be computed using `assign_to_centroids`.

    Returns
    -------
    float
        The mean silhouette score over all samples. Ranges between -1 and 1:
        - Values close to 1 indicate well-clustered points.
        - Values close to 0 indicate points on cluster boundaries.
        - Values close to -1 indicate possible misclassification.
    """
    m, n = X.shape                 # m = number of samples, n = features
    k = cntr.shape[0]              # k = number of clusters
    if c is None:
        c = assign_to_centroids(X, cntr)  # assign clusters if not provided
    
    a_i = np.zeros((m,))           # intra-cluster distance for each sample
    b_i = np.full((m,), np.inf)    # nearest-cluster distance for each sample

    # Full pairwise distance matrix: shape (m, m)
    distances = np.sqrt(np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=2))
    
    # Loop over each cluster
    for i in range(k):
        cluster_mask = (c == i)              # boolean mask for points in cluster i
        cluster_size = np.sum(cluster_mask)  # number of points in cluster i

        if cluster_size <= 1:                # singletons â†’ silhouette = 0
            a_i[cluster_mask] = 0
            continue
            
        # Compute average distance of each point in cluster i to other points in the same cluster
        a_i[cluster_mask] = np.sum(distances[cluster_mask][:, cluster_mask], axis=1) / (cluster_size - 1)
        
        # Compare to all other clusters to compute b_i
        for j in range(k):
            if j == i: 
                continue  # skip same cluster
            outer_cluster_mask = (c == j)    # mask for cluster j

            if np.any(outer_cluster_mask):   # ignore empty clusters
                # Distances between points in cluster i and points in cluster j
                inter_distances = distances[cluster_mask][:, outer_cluster_mask]
                # Average distance from each point in i to cluster j
                b_candidate = np.mean(inter_distances, axis=1)
                # Keep the minimum across all other clusters
                b_i[cluster_mask] = np.minimum(b_i[cluster_mask], b_candidate)

    # Compute silhouette score for each point
    s_i = (b_i - a_i) / np.maximum(a_i, b_i)
    # Return the mean silhouette score across all samples
    return np.mean(s_i)



silhouette(X, centroids)


from sklearn.metrics import silhouette_score



